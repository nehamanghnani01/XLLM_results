{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2c25d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b3c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a71491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_directory = '/scratch/user/nehajm'\n",
    "api_token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3d361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_llama_name = 'meta-llama/Llama-2-7b-chat-hf'  #'gpt2'\n",
    "# model_gpt_name = 'gpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1fa904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:460: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f57ce25179041ef94ac1594c0c10074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/utils/hub.py:373: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_llama_name, cache_dir=cache_directory,\n",
    "  use_auth_token=api_token, device_map='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "130f15b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:631: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_llama_name, cache_dir=cache_directory, use_auth_token=api_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a621e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original model weights\n",
    "weights = [param.data.clone() for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ccbf73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/scratch/user/nehajm/allenai___json/allenai--c4-181ebbe6122ca37f/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    }
   ],
   "source": [
    "data_files = {\"validation\": \"en/c4-validation.*.json.gz\"}\n",
    "c4_validation = load_dataset(\"allenai/c4\", data_files=data_files, split=\"validation\", cache_dir=cache_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce3f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = c4_validation['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd7c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOTAL_TOKENS =128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89791703",
   "metadata": {},
   "source": [
    "ABSMAX QUANTIZATION - for symmetric distributions in tensor values (values ranging from -ve to +ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c75b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize(X):\n",
    "    # Calculate scale\n",
    "    scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "    # Quantize\n",
    "    X_quant = (scale * X).round()    #gives you a number in the range of [-127,127]\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = X_quant / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc128424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db55c64c41fc4cdba8d21bddb4658da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_abs = AutoModelForCausalLM.from_pretrained(model_llama_name, cache_dir=cache_directory,\n",
    "  use_auth_token=api_token, device_map='auto')\n",
    "\n",
    "weights_abs = []\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d6720a",
   "metadata": {},
   "source": [
    "ZEROPOINT QUANTIZATION  - for asymmetric input distribution, like output of a ReLU function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7536b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropoint_quantize(X):\n",
    "    # Calculate value range (denominator)\n",
    "    x_range = torch.max(X) - torch.min(X)\n",
    "    x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "    # Calculate scale\n",
    "    scale = 255 / x_range\n",
    "\n",
    "    # Shift by zero-point  (to map it to the range of [-128,127])\n",
    "    zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "\n",
    "    # Quantize (scale and zeropoint measures are used for the below 2 steps of quantize (8 bit version) and dequantize (get the original tensor value back) )\n",
    "    # Scale and round the inputs\n",
    "    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90c4d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model to quantize\n",
    "model_zp = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_zp = []\n",
    "for param in model_zp.parameters():\n",
    "    _, dequantized = zeropoint_quantize(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_zp.append(dequantized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f61fb",
   "metadata": {},
   "source": [
    "INT-8 QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4826b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562c1c4ac1564d29849064b47f7f6ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_int8 = AutoModelForCausalLM.from_pretrained(model_llama_name,\n",
    "                                                  cache_dir=cache_directory,\n",
    "                                             device_map='auto',\n",
    "                                             load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231ee42",
   "metadata": {},
   "source": [
    "INT-4 QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf1bd58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0fa9f1a024f46abba91dea1cb4f3bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_int4 = AutoModelForCausalLM.from_pretrained(model_llama_name,\n",
    "                                                  cache_dir=cache_directory,\n",
    "                                             device_map='auto',\n",
    "                                             load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce34529a",
   "metadata": {},
   "source": [
    "Generating TEXT with the original and quantized models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bb42ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    output = model.generate(inputs=input_ids,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=30,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2d9f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"explain deep learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41c13348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "explain deep learning and its applications\n",
      "\n",
      "Deep learning (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate text with original and quantized models\n",
    "\n",
    "original_text = generate_text(model, input_text)\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ec0c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absmax model:\n",
      "explain deep learning to a 5 year old\n",
      " nobody understands deep learning\n",
      "Deep Learning for Kids\n",
      "Deep Learning Explained in 5 Minutes\n",
      "Deep Learning Explained in Simple Terms\n",
      "Deep Learning\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "absmax_text   = generate_text(model_abs, input_text)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da933755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeropoint model:\n",
      "explain deep learning in simple terms\n",
      "➖ 1. What is deep learning?\n",
      "Deep learning is a type of machine learning that uses artificial neural networks to model and solve complex problems. These networks are designed to mimic the structure and\n"
     ]
    }
   ],
   "source": [
    "zp_text = generate_text(model_zp, input_text)\n",
    "print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b7a7c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int8 model:\n",
      "explain deep learning for computer vision\n",
      "\n",
      "Deep learning for computer vision is a subfield of machine learning that focuses on developing algorithms and models that can be used to analyze and understand visual data from images and videos. The goal of deep learning\n"
     ]
    }
   ],
   "source": [
    "int8_text = generate_text(model_int8, input_text)\n",
    "print(f\"int8 model:\\n{int8_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a52dc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int4 model:\n",
      "explain deep learning in simple terms\n",
      " everybody can understand\n",
      "\n",
      "Deep learning is a type of machine learning that uses artificial neural networks to analyze and learn from data.\n",
      "\n",
      "Think of a neural network like a map of a city. Each\n"
     ]
    }
   ],
   "source": [
    "int4_text= generate_text(model_int4, input_text)\n",
    "print(f\"int4 model:\\n{int4_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12a6c3d",
   "metadata": {},
   "source": [
    "Generating PERPLEXITY for the Generated Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68a5194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "    # Define input_ids and target_ids\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # Loss calculation\n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Perplexity calculation\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d52f10f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  1.67\n"
     ]
    }
   ],
   "source": [
    "#Original model perplexity : \n",
    "ppl     = calculate_perplexity(model, original_text)\n",
    "print(f\"Original perplexity:  {ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bfd60ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absmax perplexity:    2.65\n"
     ]
    }
   ],
   "source": [
    "#Absmax perplexity \n",
    "ppl_abs = calculate_perplexity(model_abs, absmax_text)\n",
    "print(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97c09112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absmax perplexity:    2.21\n"
     ]
    }
   ],
   "source": [
    "#Zeropoint perplexity \n",
    "ppl_zp = calculate_perplexity(model_zp, zp_text)\n",
    "print(f\"Zeropoint perplexity:    {ppl_zp.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67050b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int8 perplexity:    1.90\n"
     ]
    }
   ],
   "source": [
    "#int8 perplexity\n",
    "ppl_int8 = calculate_perplexity(model_int8, int8_text)\n",
    "print(f\"Int8 perplexity:    {ppl_int8.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc2db3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int4 perplexity:    2.15\n"
     ]
    }
   ],
   "source": [
    "#int4 perplexity\n",
    "ppl_int4 = calculate_perplexity(model_int4, int4_text)\n",
    "print(f\"Int4 perplexity:    {ppl_int4.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b533267",
   "metadata": {},
   "source": [
    "Generate C4 DATASET PERPLEXITIES FOR ALL THE MODELS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aea9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate tokens for the C4 dataset \n",
    "def generate_tokens_c4(model):\n",
    "    # Initialize an empty list to store generated tokens\n",
    "    generated_tokens = []\n",
    "\n",
    "\n",
    "    # Set pad_token to eos_token for correct padding\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Define the maximum number of tokens you want to generate\n",
    "    MAX_TOTAL_TOKENS = 512  # Adjust this value as needed\n",
    "\n",
    "    # Tokenize the validation data and generate tokens\n",
    "    for text in validation_data:\n",
    "    # Calculate the maximum number of tokens to generate for this input\n",
    "        max_length = min(MAX_TOTAL_TOKENS - len(generated_tokens), MAX_TOTAL_TOKENS)\n",
    "        input_ids = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        generated_ids = model.generate(input_ids.input_ids, max_length=max_length)\n",
    "        generated_tokens.extend(tokenizer.decode(generated_ids[0], skip_special_tokens=True).split())\n",
    "    \n",
    "        # Check if we've reached the maximum number of tokens\n",
    "        if len(generated_tokens) >= MAX_TOTAL_TOKENS:\n",
    "            break\n",
    "\n",
    "    # Combine generated tokens into a single string\n",
    "    generated_text = \" \".join(generated_tokens)\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34a0d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 231, but `max_length` is set to 231. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 65, but `max_length` is set to 65. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 30, but `max_length` is set to 30. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 18, but `max_length` is set to 18. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 4, but `max_length` is set to 4. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 2, but `max_length` is set to 2. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 1, but `max_length` is set to 1. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#generating tokens for the original model : \n",
    "original_gen_text = generate_tokens_c4(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b769bd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 196, but `max_length` is set to 196. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 53, but `max_length` is set to 53. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 27, but `max_length` is set to 27. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 16, but `max_length` is set to 16. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#generating tokens for the absmax model : \n",
    "absmax_gen_text = generate_tokens_c4(model_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c278683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 185, but `max_length` is set to 185. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 52, but `max_length` is set to 52. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 26, but `max_length` is set to 26. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 15, but `max_length` is set to 15. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 4, but `max_length` is set to 4. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#generating tokens for the absmax model : \n",
    "zp_gen_text = generate_tokens_c4(model_zp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d23aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 215, but `max_length` is set to 215. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 60, but `max_length` is set to 60. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 29, but `max_length` is set to 29. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 18, but `max_length` is set to 18. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#generating tokens for the int8 model : \n",
    "int8_gen_text = generate_tokens_c4(model_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5246bdb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 201, but `max_length` is set to 201. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n",
      "/scratch/user/nehajm/.conda/envs/env_new_wed/lib/python3.11/site-packages/transformers/generation/utils.py:1262: UserWarning: Input length of input_ids is 56, but `max_length` is set to 56. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#generating tokens for the int4 model : \n",
    "int4_gen_text = generate_tokens_c4(model_int4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d616466d",
   "metadata": {},
   "source": [
    "Calculating PERPLEXITIES for the c4 dataset for all the models below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba32fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_from_c4(model, generated_text):\n",
    "    # Calculate perplexity\n",
    "    with torch.no_grad():\n",
    "        # Tokenize the generated text for loss calculation\n",
    "        generated_input_ids = tokenizer(generated_text, return_tensors=\"pt\").input_ids\n",
    "        # Calculate the loss using the generated input_ids as labels\n",
    "        loss = model(generated_input_ids, labels=generated_input_ids).loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59ac8d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:  13.67\n"
     ]
    }
   ],
   "source": [
    "#Original model perplexity on the c4 dataset \n",
    "orginal_gen_text_ppl = calculate_perplexity_from_c4(model, original_gen_text)\n",
    "print(f\"Original model:  {orginal_gen_text_ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed45d33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absmax model:  11.90\n"
     ]
    }
   ],
   "source": [
    "#Absmax model perplexity on the c4 dataset \n",
    "absmax_gen_text_ppl = calculate_perplexity_from_c4(model_abs, absmax_gen_text)\n",
    "print(f\"Absmax model:  {absmax_gen_text_ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3acee29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeropoint model:  12.00\n"
     ]
    }
   ],
   "source": [
    "#Zeropoint model perplexity on the c4 dataset \n",
    "zp_gen_text_ppl = calculate_perplexity_from_c4(model_zp, zp_gen_text)\n",
    "print(f\"Zeropoint model:  {zp_gen_text_ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3f40377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int 8 model:  12.79\n"
     ]
    }
   ],
   "source": [
    "#Int8 model perplexity on the c4 dataset \n",
    "int8_gen_text_ppl = calculate_perplexity_from_c4(model_int8, int8_gen_text)\n",
    "print(f\"Int 8 model:  {int8_gen_text_ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "995b878a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int 4 model:  12.41\n"
     ]
    }
   ],
   "source": [
    "#Int4 model perplexity on the c4 dataset \n",
    "int4_gen_text_ppl = calculate_perplexity_from_c4(model_int4, int4_gen_text)\n",
    "print(f\"Int 4 model:  {int4_gen_text_ppl.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd4c417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
